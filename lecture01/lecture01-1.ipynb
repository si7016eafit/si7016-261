{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#\n",
        "# Universidad EAFIT \n",
        "# 2026-1\n",
        "#\n",
        "# Lab – Lecture 01 (Parte 1): Representación clásica del texto\n",
        "\n",
        "Este notebook apoya la clase 01 del curso **Procesamiento de Lenguaje Natural Aplicado**. \n",
        "Exploraremos técnicas fundamentales de preprocesamiento y representación de texto:\n",
        "\n",
        "### ejemplos:\n",
        "- Carga de documentos\n",
        "- Tokenización, stopwords, lematización\n",
        "- Representación con Bag of Words y TF-IDF\n",
        "- Cálculo de similitud entre documentos\n",
        "- Visualización con PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWBFHbzS20OQ",
        "outputId": "6688528e-4d45-4279-9066-12dc0ed9f90f"
      },
      "outputs": [],
      "source": [
        "#configuración en google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztaFc9Rk3xx3",
        "outputId": "79655aa2-c8a7-4211-d15e-6bf2405b34d5"
      },
      "outputs": [],
      "source": [
        "!pip3 install nltk\n",
        "!pip3 install pandas\n",
        "!pip3 install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importación de librerías\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from nltk.corpus import stopwords, reuters\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# cargar las librerias necesarias\n",
        "## 1. nltk para 'procesamiento natural del lenguaje'\n",
        "## 2. pandas para procesamiento de dataframes, muy usado en preparación de datos\n",
        "## 3. re - expresiones regulares\n",
        "## 4. numpy, codecs, etc - otras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCczY-GZ4hLN",
        "outputId": "2d553b2a-bbfa-4d93-cc3f-d476506248d6"
      },
      "outputs": [],
      "source": [
        "!ls 'gdrive/MyDrive/si7016-261/datasets/gutenberg-es/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3mMCS952X2g"
      },
      "outputs": [],
      "source": [
        "# directorios (path) de entrada y salida:\n",
        "# \n",
        "path_in=\"gdrive/MyDrive/si7016-261/datasets/gutenberg-es/\"\n",
        "path_out=\"gdrive/MyDrive/si7016-261/out/\"\n",
        "filenametxt='don-quijote.txt'\n",
        "filenamecleantxt='don-quijote_clean.txt'\n",
        "filenamecsv='don-quijote.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2PK3gGK2X2j",
        "outputId": "b3f87301-8ba4-4bd5-d073-772588efede2"
      },
      "outputs": [],
      "source": [
        "# corpus de nltk para 'tokenizer' y 'stopwords'\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ejemplo de como nltk tokeniza:\n",
        "texto=\"texto libre que permite crear     hiso20091iras epor--4 no s#e preocupe \\n hola mundo cruel\"\n",
        "tokens = nltk.word_tokenize(texto)\n",
        "print(len(tokens))\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# note la estrategia de tokenizar con sentencias simples de python, \n",
        "# ¿ cual le parece mejor?\n",
        "# y note la diferencia entre .split() y .split(' ')\n",
        "texto=\"texto libre que permite crear     hiso20091iras            epor--4 no s#e preocupe \\n hola mundo cruel\"\n",
        "tokens = texto.split()\n",
        "print(len(tokens))\n",
        "print(tokens)\n",
        "tokens = texto.split(' ')\n",
        "print(len(tokens))\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# stopwords en nltk\n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "stop_words_nltk = set(stopwords.words('spanish'))\n",
        "stop_words_nltk_en = set(stopwords.words('english'))\n",
        "print(len(stop_words_nltk_en))\n",
        "print(stop_words_nltk_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# permite verificar en nltk si un token pertenece a diccionario de un idioma, en este caso a 'english'\n",
        "from nltk.corpus import words as voc_en\n",
        "\n",
        "x = len(voc_en.words())\n",
        "print('tamaño del diccionario en inglés del nltk: ',x)\n",
        "\n",
        "# verifica si una palabra pertenece al diccionario:\n",
        "w = \"house\"\n",
        "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words_nltk_en):\n",
        "    print(w,\" true\")\n",
        "else:\n",
        "    print(w,\" false\")\n",
        "    \n",
        "w = \"eafit\"\n",
        "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words_nltk_en):\n",
        "    print(w,\" true\")\n",
        "else:\n",
        "    print(w,\" false\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# leer un archivo de ejemplo en .txt\n",
        "input_file = open(path_in+filenametxt, \"r\", encoding='iso-8859-1')\n",
        "filedata = input_file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenización de n-grams usando Scikit-learn\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Texto de ejemplo\n",
        "text = [\"El gato está durmiendo tranquilamente en el sofá\"]\n",
        "\n",
        "# Crear un CountVectorizer con diferentes valores de n\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3))  # Unigramas, bigramas y trigramas\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Mostrar los n-grams generados\n",
        "print(\"N-grams generados:\", vectorizer.get_feature_names_out())\n",
        "print(\"Matriz de conteo:\\n\", X.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenización de n-grams manual\n",
        "def generate_ngrams(text, n):\n",
        "    \"\"\"\n",
        "    Genera una lista de n-grams a partir de un texto.\n",
        "    \"\"\"\n",
        "    tokens = text.split()  # Tokenización básica por espacios\n",
        "    ngrams = [\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "    return ngrams\n",
        "\n",
        "# Texto de ejemplo\n",
        "text = \"El gato está durmiendo tranquilamente en el sofá\"\n",
        "\n",
        "# Generar n-grams para diferentes valores de n\n",
        "for n in range(1, 4):  # Unigramas, bigramas, trigramas\n",
        "    print(f\"{n}-grams:\", generate_ngrams(text, n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# opción 1:\n",
        "# TOKENIZAR con .split(), \n",
        "# ELIMINAR tokens de long = 1\n",
        "# ELIMINAR caracteres que no sean alfanumericos y pasar todo a minuscula\n",
        "# REMOVER stop words con nltk\n",
        "# graficar los 20 términos más frecuentes:\n",
        "\n",
        "tokens = filedata.split()\n",
        "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
        "# tokens=[word for word in tokens if word.isalpha()] si en vez de re.sub(r'[^A-Za-z0-9]+','',w) hace esto, que pasa?\n",
        "tokens = [w.lower() for w in tokens if len(w)>1]\n",
        "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "topwords = fdist.most_common(20)\n",
        "print (topwords)\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# opción 2:\n",
        "# TOKENIZAR con nltk, \n",
        "# ELIMINAR tokens de long = 1\n",
        "# ELIMINAR caracteres que no sean alfanumericos\n",
        "# REMOVER stop words\n",
        "# graficar los 20 términos más frecuentes:\n",
        "\n",
        "tokens = nltk.word_tokenize(filedata)\n",
        "tokens = [w.lower() for w in tokens if len(w)>1]\n",
        "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
        "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "print (topwords)\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stemming con NLTK\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "# probar cada una de las siguientes opciones: porter y lancaster.\n",
        "#tokens = [porter.stem(w) for w in tokens]\n",
        "tokens = [lancaster.stem(w) for w in tokens]\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lemmatization con NLTK\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# probar cada una de las siguientes opciones: \n",
        "#tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens ]\n",
        "tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens ]\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# volver a leer el archivo ejemplo en .txt\n",
        "#input_file = open(path_in+filenametxt, \"r\",encoding='iso-8859-1')\n",
        "input_file = open(path_in+filenametxt, \"r\")\n",
        "output_file_clean = open(path_out+filenamecleantxt, \"w\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for line in input_file:\n",
        "    line_clean = \"\"\n",
        "    tokens = nltk.word_tokenize(line)\n",
        "    tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
        "    tokens = [w.lower() for w in tokens if len(w)>1]\n",
        "    tokens = [w for w in tokens if w.isalpha()]\n",
        "    tokens = [w for w in tokens if w not in stop_words_nltk]\n",
        "    #tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens]\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens]\n",
        "\n",
        "    #tokens = [porter.stem(w) for w in tokens]\n",
        "    tokens = [lancaster.stem(w) for w in tokens]\n",
        "    \n",
        "    for w in tokens:\n",
        "        line_clean=line_clean+w+\" \"\n",
        "            \n",
        "    if (line_clean!=\"\"):\n",
        "        line_clean=line_clean+\"\\n\"\n",
        "        output_file_clean.write(line_clean)\n",
        "output_file_clean.close()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_file_clean = open(path_out+filenamecleantxt, \"r\", encoding='iso-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filedata = input_file_clean.read()\n",
        "tokens = filedata.split()\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_freq = fdist.most_common(len(fdist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open(path_out+filenamecsv, 'w') as csvFile:\n",
        "    writer = csv.writer(csvFile)\n",
        "    writer.writerow([\"word\", \"frecuency\"])\n",
        "    writer.writerows(word_freq)\n",
        "\n",
        "csvFile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract top 30 words\n",
        "top_words = word_freq[:20]\n",
        "print(top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(top_words)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x,y = zip(*top_words)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df = pd.DataFrame(top_words)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(df[0],df[1])\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Word\")\n",
        "plt.ylabel(\"frecuency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# representacion BoW\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "corpus = [\n",
        "    \"El gato está durmiendo\",\n",
        "    \"El perro está jugando\",\n",
        "    \"El gato y el perro son amigos\",\n",
        "    \"A los gatos les gusta dormir todo el día\"\n",
        "]\n",
        "\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_bow = bow_vectorizer.fit_transform(corpus)\n",
        "df_bow = pd.DataFrame(X_bow.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBag of Words\")\n",
        "print(df_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# representación TF-IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Ejemplo de documentos\n",
        "corpus = [\n",
        "    \"El gato está durmiendo\",\n",
        "    \"El perro está jugando\",\n",
        "    \"El gato y el perro son amigos\",\n",
        "    \"A los gatos les gusta dormir todo el día\"\n",
        "]\n",
        "\n",
        "# using default tokenizer in TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(min_df=1, max_df=0.5, ngram_range=(1, 2))\n",
        "features = tfidf.fit_transform(corpus)\n",
        "df_tfidf = pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names_out()\n",
        ")\n",
        "\n",
        "print(\"\\nTF-IDF\")\n",
        "print(df_tfidf.round(2))    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Qué aprendimos en este lab\n",
        "- El texto crudo no es usable directamente por modelos\n",
        "- El preprocesamiento define el espacio de representación\n",
        "- Decisiones simples (lowercase, stopwords) tienen impacto aguas abajo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Reto corto – LAB 1 (apropiación conceptual)\n",
        "\n",
        "- Toma el otros archivos diferentes del directorio 'datasets' y ejecuta dos pipelines distintos:\n",
        "- Con stopwords\n",
        "- Sin stopwords\n",
        "- Pregunta:\n",
        "- ¿En qué tipo de tareas conservarías las stopwords y por qué?\n",
        "- Da un ejemplo concreto (clasificación, QA, IR, etc.).\n",
        "\n",
        "- Responde aca mismo:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab1-nltk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs224n",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
