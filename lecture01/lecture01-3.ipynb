{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "# Universidad EAFIT \n",
    "# 2026-1\n",
    "#\n",
    "# Lab – Lecture 01 (Parte 3): Representación de texto y vectorización\n",
    "\n",
    "Este notebook contiene ejemplos prácticos de preprocesamiento y representación clásica de texto para la clase 01 del curso de NLP Aplicado.\n",
    "\n",
    "## ejemplos\n",
    "- Aplicar técnicas de preprocesamiento: tokenización, stopwords, lematización.\n",
    "- Representar texto con Bag of Words (BoW) y TF-IDF.\n",
    "- Calcular similitud entre documentos.\n",
    "- Visualizar con PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías necesarias\n",
    "!pip install nltk scikit-learn matplotlib seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de un corpus de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import random\n",
    "\n",
    "# Tomamos una muestra de documentos\n",
    "docs_ids = reuters.fileids()\n",
    "sample_ids = random.sample(docs_ids, 10)\n",
    "documents = [reuters.raw(doc_id) for doc_id in sample_ids]\n",
    "documents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento del texto\n",
    "- Tokenización\n",
    "- Eliminación de stopwords\n",
    "- Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "processed_docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Representación del texto: BoW y TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer_bow = CountVectorizer(max_features=100)\n",
    "X_bow = vectorizer_bow.fit_transform(processed_docs)\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=100)\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(processed_docs)\n",
    "\n",
    "X_bow.toarray()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Similitud entre documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim_matrix = cosine_similarity(X_tfidf)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(sim_matrix, annot=True, cmap='Blues')\n",
    "plt.title(\"Similitud Coseno entre Documentos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualización con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c='blue')\n",
    "plt.title(\"Proyección PCA de Documentos (TF-IDF)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f6eee",
   "metadata": {},
   "source": [
    "- Reto corto – LAB 3\n",
    "\n",
    "- Modifica el pipeline para:\n",
    "\n",
    "- Cambiar la función de similitud (coseno vs euclidiana)\n",
    "- Cambiar la representación (BoW vs TF-IDF)\n",
    "\n",
    "- Responde:\n",
    "\n",
    "- ¿Cambian los documentos recuperados?\n",
    "- ¿Por qué una métrica funciona mejor que otra según el caso?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
